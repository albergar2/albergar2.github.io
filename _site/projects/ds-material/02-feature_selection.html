<!DOCTYPE html>
<html lang="en-US">
  <head>
    <title>Alberto García | Data Scientist</title>

    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Feature Selection Guide | Alberto García Hernández</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Feature Selection Guide" />
<meta name="author" content="Alberto García Hernández" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Data Scientist" />
<meta property="og:description" content="Data Scientist" />
<link rel="canonical" href="http://localhost:4000/projects/ds-material/02-feature_selection.html" />
<meta property="og:url" content="http://localhost:4000/projects/ds-material/02-feature_selection.html" />
<meta property="og:site_name" content="Alberto García Hernández" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-01T00:00:00+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Feature Selection Guide" />
<script type="application/ld+json">
{"url":"http://localhost:4000/projects/ds-material/02-feature_selection.html","headline":"Feature Selection Guide","dateModified":"2021-01-01T00:00:00+01:00","datePublished":"2021-01-01T00:00:00+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/projects/ds-material/02-feature_selection.html"},"author":{"@type":"Person","name":"Alberto García Hernández"},"description":"Data Scientist","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=4f57e7ce8beaa922ed348096ac56d663a50ae2aa">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
  
  </head>
  <body>
    <div class="wrapper">
      
      <h1>Feature Selection Guide</h1>
      <p class="view">By Alberto García Hernández</p>
      <p class="view">1 January 2021</p>
      
      <p><a href="../../">Back</a></p>
<h4 id="download-notebook"><a href="https://github.com/albergar2/data_science_material/blob/master/02_Feature_Selection.ipynb">Download Notebook</a></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div>

<h2 id="1-filter">1. Filter</h2>

<p>Select subsets of features according to their relationship with the target variable. It is based on a statistics approach.</p>

<p><img src="/projects/ds-material/img-02-feature_selection/feng_process.png" alt="Drag Racing" /></p>

<h3 id="11-categorical-input-chi-squared">1.1 Categorical Input: Chi-Squared</h3>

<p>Statistical test to see the independence of 2 categorical variables, the variables that are independent of the target variable would be the candidates to be eliminated.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">chi2</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OrdinalEncoder</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cat_data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'data/categorical_input.csv'</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">cat_data</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>'40-49'</td>
      <td>'premeno'</td>
      <td>'15-19'</td>
      <td>'0-2'</td>
      <td>'yes'</td>
      <td>'3'</td>
      <td>'right'</td>
      <td>'left_up'</td>
      <td>'no'</td>
      <td>'recurrence-events'</td>
    </tr>
    <tr>
      <th>1</th>
      <td>'50-59'</td>
      <td>'ge40'</td>
      <td>'15-19'</td>
      <td>'0-2'</td>
      <td>'no'</td>
      <td>'1'</td>
      <td>'right'</td>
      <td>'central'</td>
      <td>'no'</td>
      <td>'no-recurrence-events'</td>
    </tr>
    <tr>
      <th>2</th>
      <td>'50-59'</td>
      <td>'ge40'</td>
      <td>'35-39'</td>
      <td>'0-2'</td>
      <td>'no'</td>
      <td>'2'</td>
      <td>'left'</td>
      <td>'left_low'</td>
      <td>'no'</td>
      <td>'recurrence-events'</td>
    </tr>
    <tr>
      <th>3</th>
      <td>'40-49'</td>
      <td>'premeno'</td>
      <td>'35-39'</td>
      <td>'0-2'</td>
      <td>'yes'</td>
      <td>'3'</td>
      <td>'right'</td>
      <td>'left_low'</td>
      <td>'yes'</td>
      <td>'no-recurrence-events'</td>
    </tr>
    <tr>
      <th>4</th>
      <td>'40-49'</td>
      <td>'premeno'</td>
      <td>'30-34'</td>
      <td>'3-5'</td>
      <td>'yes'</td>
      <td>'2'</td>
      <td>'left'</td>
      <td>'right_up'</td>
      <td>'no'</td>
      <td>'recurrence-events'</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="p">{</span><span class="s">'missings'</span><span class="p">:</span><span class="n">cat_data</span><span class="p">.</span><span class="n">isnull</span><span class="p">().</span><span class="nb">sum</span><span class="p">(),</span> 
                   <span class="s">'percent_missing'</span><span class="p">:</span> <span class="n">cat_data</span><span class="p">.</span><span class="n">isnull</span><span class="p">().</span><span class="nb">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_data</span><span class="p">)},</span> 
             <span class="n">index</span><span class="o">=</span><span class="n">cat_data</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>missings</th>
      <th>percent_missing</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">cat_data</span><span class="p">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="n">cat_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">cat_data</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">fillna</span><span class="p">(</span><span class="n">cat_data</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">mode</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">cat_data</span><span class="p">.</span><span class="n">values</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Encode categories based on appearance
</span><span class="n">oe</span> <span class="o">=</span> <span class="n">OrdinalEncoder</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">oe</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Encode target for binary clasiffication
</span><span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">le</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fs</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">score_func</span><span class="o">=</span><span class="n">chi2</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="s">'all'</span><span class="p">)</span>
<span class="n">fs</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">X_fs</span> <span class="o">=</span> <span class="n">fs</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># The greater the scores are the better, because more dependant are from each other
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">fs</span><span class="p">.</span><span class="n">scores_</span><span class="p">)):</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'Feature %d: %f'</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">fs</span><span class="p">.</span><span class="n">scores_</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

<span class="n">plt</span><span class="p">.</span><span class="n">bar</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">fs</span><span class="p">.</span><span class="n">scores_</span><span class="p">))],</span> <span class="n">fs</span><span class="p">.</span><span class="n">scores_</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Feature 0: 0.563284
Feature 1: 0.712378
Feature 2: 5.534357
Feature 3: 84.840985
Feature 4: 17.621176
Feature 5: 13.272755
Feature 6: 0.522775
Feature 7: 0.425232
Feature 8: 8.197246
</code></pre></div></div>

<p><img src="/projects/ds-material/img-02-feature_selection/output_13_1.png" alt="png" /></p>

<h3 id="12-categorical-input-mutual-information">1.2 Categorical Input: Mutual Information</h3>

<p>It measures the mutual dependence of two variables, that is, it measures the reduction of the uncertainty (entropy) of a random variable, X, due to the knowledge of the value of another random variable Y. For example, if X and Y are independent, then knowing X does not give information about Y and vice versa, so their mutual information is zero.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OrdinalEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">mutual_info_classif</span>

<span class="n">cat_data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'data/categorical_input.csv'</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">cat_data</span><span class="p">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="n">cat_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">cat_data</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">fillna</span><span class="p">(</span><span class="n">cat_data</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">mode</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">cat_data</span><span class="p">.</span><span class="n">values</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Encode categories based on appearance
</span><span class="n">oe</span> <span class="o">=</span> <span class="n">OrdinalEncoder</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">oe</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Encode target for binary classification
</span><span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">le</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="n">fs</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">score_func</span><span class="o">=</span><span class="n">mutual_info_classif</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="s">'all'</span><span class="p">)</span>
<span class="n">fs</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">X_fs</span> <span class="o">=</span> <span class="n">fs</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># The greater the scores are the better, because more dependant are from each other
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">fs</span><span class="p">.</span><span class="n">scores_</span><span class="p">)):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Feature %d: %f'</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">fs</span><span class="p">.</span><span class="n">scores_</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

<span class="n">plt</span><span class="p">.</span><span class="n">bar</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">fs</span><span class="p">.</span><span class="n">scores_</span><span class="p">))],</span> <span class="n">fs</span><span class="p">.</span><span class="n">scores_</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Feature 0: 0.068533
Feature 1: 0.000000
Feature 2: 0.000000
Feature 3: 0.074196
Feature 4: 0.054834
Feature 5: 0.117169
Feature 6: 0.000000
Feature 7: 0.000000
Feature 8: 0.000000
</code></pre></div></div>

<p><img src="/projects/ds-material/img-02-feature_selection/output_16_1.png" alt="png" /></p>

<h3 id="13-numerical-input-anova">1.3 Numerical Input: ANOVA</h3>

<p>ANOVA: means analysis of variance. It is a statistical technique that indicates whether two variables (one independent and one dependent) are related based on whether the means of the dependent variable are different in the categories or groups of the independent variable. In other words, it indicates whether the means between two or more groups are similar or different.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">f_classif</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OrdinalEncoder</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'data/numerical_input.csv'</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">num_data</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6</td>
      <td>148</td>
      <td>72</td>
      <td>35</td>
      <td>0</td>
      <td>33.6</td>
      <td>0.627</td>
      <td>50</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>85</td>
      <td>66</td>
      <td>29</td>
      <td>0</td>
      <td>26.6</td>
      <td>0.351</td>
      <td>31</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8</td>
      <td>183</td>
      <td>64</td>
      <td>0</td>
      <td>0</td>
      <td>23.3</td>
      <td>0.672</td>
      <td>32</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>89</td>
      <td>66</td>
      <td>23</td>
      <td>94</td>
      <td>28.1</td>
      <td>0.167</td>
      <td>21</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>137</td>
      <td>40</td>
      <td>35</td>
      <td>168</td>
      <td>43.1</td>
      <td>2.288</td>
      <td>33</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">num_data</span><span class="p">.</span><span class="n">values</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">le</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="n">fs</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">score_func</span><span class="o">=</span><span class="n">f_classif</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="s">'all'</span><span class="p">)</span>
<span class="n">fs</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">X_fs</span> <span class="o">=</span> <span class="n">fs</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># The greater the scores are the better, because more dependant are from each other
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">fs</span><span class="p">.</span><span class="n">scores_</span><span class="p">)):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Feature %d: %f'</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">fs</span><span class="p">.</span><span class="n">scores_</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

<span class="n">plt</span><span class="p">.</span><span class="n">bar</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">fs</span><span class="p">.</span><span class="n">scores_</span><span class="p">))],</span> <span class="n">fs</span><span class="p">.</span><span class="n">scores_</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Feature 0: 1.468332
Feature 1: 0.784878
Feature 2: 4.925982
Feature 3: 26.489526
Feature 4: 23.563620
Feature 5: 27.964633
Feature 6: 0.980135
Feature 7: 0.638976
Feature 8: 11.096221
</code></pre></div></div>

<p><img src="/projects/ds-material/img-02-feature_selection/output_21_1.png" alt="png" /></p>

<h3 id="14-pearsons-correlation">1.4 Pearson’s Correlation</h3>

<p>Correlation is a measure of how two variables change together. Perhaps the most common is the Pearson correlation which assumes a Gaussian distribution for each variable.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">f_regression</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OrdinalEncoder</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">num_data</span><span class="p">.</span><span class="n">values</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">fs</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">score_func</span><span class="o">=</span><span class="n">f_regression</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="s">'all'</span><span class="p">)</span>
<span class="n">fs</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">X_fs</span> <span class="o">=</span> <span class="n">fs</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># The greater the scores are the better, because more dependant are from each other
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">fs</span><span class="p">.</span><span class="n">scores_</span><span class="p">)):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Feature %d: %f'</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">fs</span><span class="p">.</span><span class="n">scores_</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

<span class="n">plt</span><span class="p">.</span><span class="n">bar</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">fs</span><span class="p">.</span><span class="n">scores_</span><span class="p">))],</span> <span class="n">fs</span><span class="p">.</span><span class="n">scores_</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Feature 0: 39.670227
Feature 1: 213.161752
Feature 2: 3.256950
Feature 3: 4.304381
Feature 4: 13.281108
Feature 5: 71.772072
Feature 6: 23.871300
Feature 7: 46.140611
</code></pre></div></div>

<p><img src="/projects/ds-material/img-02-feature_selection/output_24_1.png" alt="png" /></p>

<h2 id="2-wrapper">2. Wrapper</h2>

<p>Select subsets of features based on their predictive power.</p>

<h3 id="21-recursive-feature-elimination-rfe">2.1 Recursive Feature Elimination (RFE)</h3>

<p>RFE is popular because it is easy to set up and use and because it is effective in selecting those features in a training data set that are most relevant to predicting the target variable.</p>

<p>There are two important configuration options when using RFE:</p>
<ul>
  <li>the choice of the number of features to select</li>
  <li>the choice of the algorithm used to help choose the features.</li>
</ul>

<p>Both of these hyperparameters can be explored, although the performance of the method does not depend greatly on these hyperparameters being set correctly.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RepeatedStratifiedKFold</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFE</span><span class="p">,</span> <span class="n">RFECV</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="c1">#from sklearn.ensemble import RandomForestClassifier
</span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># define dataset
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># summarize the dataset
</span><span class="k">print</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1">#rfe = RFECV(estimator=DecisionTreeRegressor()) # We can select n_features automatically
#rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)
</span><span class="n">rfe</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">LogisticRegression</span><span class="p">(),</span> <span class="n">n_features_to_select</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">rfe</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Column: %d, Selected=%s, Rank: %d'</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">rfe</span><span class="p">.</span><span class="n">support_</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">rfe</span><span class="p">.</span><span class="n">ranking_</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
    
<span class="c1"># We can use any other model such as DecisionTreeClassifier, RandomForestClassifier
</span><span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[(</span><span class="s">'s'</span><span class="p">,</span><span class="n">rfe</span><span class="p">),(</span><span class="s">'m'</span><span class="p">,</span><span class="n">model</span><span class="p">)])</span>

<span class="c1"># evaluate model
</span><span class="n">cv</span> <span class="o">=</span> <span class="n">RepeatedStratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_repeats</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">n_scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">'accuracy'</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Accuracy: %.3f (%.3f)'</span> <span class="o">%</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">n_scores</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">std</span><span class="p">(</span><span class="n">n_scores</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(1000, 10) (1000,)
Column: 0, Selected=True, Rank: 1
Column: 1, Selected=False, Rank: 6
Column: 2, Selected=True, Rank: 1
Column: 3, Selected=True, Rank: 1
Column: 4, Selected=True, Rank: 1
Column: 5, Selected=True, Rank: 1
Column: 6, Selected=False, Rank: 4
Column: 7, Selected=False, Rank: 2
Column: 8, Selected=False, Rank: 5
Column: 9, Selected=False, Rank: 3
Accuracy: 0.851 (0.024)
</code></pre></div></div>

<h2 id="3-intrinsinc">3. Intrinsinc</h2>

<p>Algorithms perform automatic feature selection during training.</p>

<p>Feature importance is the technique that assigns a score to each input depending on how useful it is in predicting a target variable</p>

<h3 id="31-regression">3.1 Regression</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1">#model = LinearRegression()
</span><span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">()</span>

<span class="c1"># fit the model
</span><span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1">#importance = model.coef_
</span><span class="n">importance</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">feature_importances_</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">importance</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Feature: %0d, Score: %.5f'</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">v</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="n">bar</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">importance</span><span class="p">))],</span> <span class="n">importance</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Feature: 0, Score: 0.00252
Feature: 1, Score: 0.00436
Feature: 2, Score: 0.00236
Feature: 3, Score: 0.00120
Feature: 4, Score: 0.51660
Feature: 5, Score: 0.43811
Feature: 6, Score: 0.02780
Feature: 7, Score: 0.00321
Feature: 8, Score: 0.00273
Feature: 9, Score: 0.00111
</code></pre></div></div>

<p><img src="/projects/ds-material/img-02-feature_selection/output_34_1.png" alt="png" /></p>

<h3 id="32-classification">3.2 Classification</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1">#model = LogisticRegression()
</span><span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">importance</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1">#importance = model.feature_importances_
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">importance</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Feature: %0d, Score: %.5f'</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">v</span><span class="p">))</span>

<span class="c1"># This is a classification problem with classes 0 and 1.
# Positive scores indicate a characteristic that predicts class 1.
# Negative scores indicate a characteristic that predicts class 0.
</span><span class="n">plt</span><span class="p">.</span><span class="n">bar</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">importance</span><span class="p">))],</span> <span class="n">importance</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Feature: 0, Score: 0.16320
Feature: 1, Score: -0.64301
Feature: 2, Score: 0.48497
Feature: 3, Score: -0.46190
Feature: 4, Score: 0.18432
Feature: 5, Score: -0.11978
Feature: 6, Score: -0.40602
Feature: 7, Score: 0.03772
Feature: 8, Score: -0.51785
Feature: 9, Score: 0.26540
</code></pre></div></div>

<p><img src="/projects/ds-material/img-02-feature_selection/output_36_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

      <footer>
        
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
  </body>
</html>
