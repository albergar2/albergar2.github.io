<!DOCTYPE html>
<html lang="en-US">
  <head>
    <title>Alberto García | Data Scientist</title>

    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Supervised Machine Learning: Decision Tree | Alberto García Hernández</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Supervised Machine Learning: Decision Tree" />
<meta name="author" content="Alberto García Hernández" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Data Scientist" />
<meta property="og:description" content="Data Scientist" />
<link rel="canonical" href="http://localhost:4000/projects/intro-ml/03-decision_tree.html" />
<meta property="og:url" content="http://localhost:4000/projects/intro-ml/03-decision_tree.html" />
<meta property="og:site_name" content="Alberto García Hernández" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-01T00:00:00+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Supervised Machine Learning: Decision Tree" />
<script type="application/ld+json">
{"url":"http://localhost:4000/projects/intro-ml/03-decision_tree.html","headline":"Supervised Machine Learning: Decision Tree","dateModified":"2021-01-01T00:00:00+01:00","datePublished":"2021-01-01T00:00:00+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/projects/intro-ml/03-decision_tree.html"},"author":{"@type":"Person","name":"Alberto García Hernández"},"description":"Data Scientist","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=8070621d74c168a42606ab654f266849f1fbb924">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
  
  </head>
  <body>
    <div class="wrapper">
      
      <h1>Supervised Machine Learning: Decision Tree</h1>
      <p class="view">By Alberto García Hernández</p>
      <p class="view">1 January 2021</p>
      
      <p><a href="../../">Back</a></p>
<h4 id="download-notebook"><a href="https://github.com/albergar2/data_science_material/blob/master/ML/supervised/03-decision_tree.ipynb">Download Notebook</a></h4>

<p>A decision tree is a type of supervised learning algorithm that can be used in classification as well as regressor problems. The input to a decision tree can be both continuous as well as categorical. The decision tree works on an if-then statement.</p>

<ol>
  <li>Initially all the training set is considered as a root.</li>
  <li>Feature values are preferred to be categorical, if continuous then they are discretized.</li>
  <li>Records are distributed recursively on the basis of attribute values.</li>
  <li>Which attributes are considered to be in root node or internal node is done by using a statistical approach.</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">export_graphviz</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span> 
<span class="kn">from</span> <span class="nn">pydot</span> <span class="kn">import</span> <span class="n">graph_from_dot_data</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">io</span> <span class="kn">import</span> <span class="n">StringIO</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
</code></pre></div></div>

<h2 id="1-load-data">1. Load Data</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">iris</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">iris</span><span class="p">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">Categorical</span><span class="p">.</span><span class="n">from_codes</span><span class="p">(</span><span class="n">iris</span><span class="p">.</span><span class="n">target</span><span class="p">,</span> <span class="n">iris</span><span class="p">.</span><span class="n">target_names</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">y</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>setosa</th>
      <th>versicolor</th>
      <th>virginica</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="2-decision-tree">2. Decision Tree</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s">'gini'</span><span class="p">,</span> 
                            <span class="n">max_depth</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> 
                            <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
                            <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">ccp_alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">dt</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DecisionTreeClassifier(ccp_alpha=0.1, min_samples_leaf=5, min_samples_split=10)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dot_data</span> <span class="o">=</span> <span class="n">StringIO</span><span class="p">()</span>
<span class="n">export_graphviz</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">out_file</span><span class="o">=</span><span class="n">dot_data</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">iris</span><span class="p">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="p">)</span> <span class="o">=</span> <span class="n">graph_from_dot_data</span><span class="p">(</span><span class="n">dot_data</span><span class="p">.</span><span class="n">getvalue</span><span class="p">())</span>
<span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="p">.</span><span class="n">create_png</span><span class="p">())</span>
</code></pre></div></div>

<p><img src="img-03-decision_tree/output_10_0.png" alt="png" /></p>

<h2 id="3-key-concepts">3. Key concepts</h2>

<p>There are different attributes which define the split of nodes in a decision tree.</p>

<ul>
  <li><strong>Entropy</strong>: measure of the amount of uncertainty (impurity) in the dataset. Entropy varies from 0 to 1. 0 if all the data belong to a single class and 1 if the class distribution is equal. In this way, entropy will give a measure of impurity in the dataset.</li>
  <li><strong>Information Gain</strong>: based on the decrease in entropy after a data-set is split on an attribute. A decision tree tries to find the attribute that returns the highest information gain.</li>
  <li><strong>GINI score</strong>: sum of the square of probability for success and failure (p2+q2).</li>
  <li><strong>Maximum depth</strong>: a limit to stop the further splitting of nodes when the specified tree depth has been reached, its use its not recommended as the Information Gain won’t be zero on every leaf. This is a BAD way to limit the tree.</li>
  <li><strong>Minimum split size</strong>: a limit to stop the further splitting of nodes when the number of observations in the node is lower than the minimum split size. When a leaf contains too few observations, further splitting will result in overfitting. This is a GOOD way to limit the tree.</li>
  <li><strong>Minimum leaf size</strong>: a limit to split a node when the number of observations in one of the child nodes is lower than the minimum leaf size.</li>
  <li><strong>Pruning</strong>: mostly done to reduce the chances of overfitting the tree to the training data and reduce the overall complexity of the tree.
    <ul>
      <li><strong>Pre-prunnig</strong> (early stopping criteria): criteria are set as parameter and the tree stops growing when it meets any of these pre-pruning criteria, or it discovers the pure classes.</li>
      <li><strong>Post-prunning</strong>: allow the decision tree to grow fully and observe the CP (Complexity Parameter value, in order to prune the tree later on the optimal CP.</li>
    </ul>
  </li>
</ul>

<p>There are few algorithms to find the optimal split:</p>

<ul>
  <li><strong>ID3 Algorithm</strong> (Iterative Dichotomiser 3): This solution uses Entropy and Information gain as metrics to form a better decision tree. The attribute with the highest information gain is used as a root node, and a similar approach is followed after that. A leaf node is decided when entropy is zero.
    <ol>
      <li>Compute the entropy for the dataset</li>
      <li>For every attribute:
        <ul>
          <li>Calculate entropy for all categorical values.</li>
          <li>Take average information entropy for the attribute.</li>
          <li>Calculate gain for the current attribute.</li>
        </ul>
      </li>
      <li>Pick the attribute with the highest information gain.</li>
      <li>Repeat until we get the desired tree.</li>
    </ol>
  </li>
  <li><strong>CART Algorithm</strong> (Classification and Regression trees): uses the GINI index as a cost function to evaluate split in a dataset.
    <ol>
      <li>Calculate Gini for subnodes, using formula: sum of the square of probability for success and failure (p2+q2).</li>
      <li>Calculate Gini for split using weighted Gini score of each node of that split. Choose the split based on higher Gini value</li>
    </ol>

    <p><img src="attachment:388db081-16d4-471b-afe7-1349ffc26b7f.png" alt="imagen.png" /></p>
  </li>
  <li><strong>Split on Gender</strong>:
    <ul>
      <li>Gini for sub-node Female = (0.2)<em>(0.2)+(0.8)</em>(0.8)=0.68</li>
      <li>Gini for sub-node Male = (0.65)<em>(0.65)+(0.35)</em>(0.35)=0.55</li>
      <li>Weighted Gini for Split Gender = (10/30)<em>0.68+(20/30)</em>0.55 = 0.59</li>
    </ul>
  </li>
  <li>Split on Class:
    <ul>
      <li>Gini for sub-node Class IX = (0.43)<em>(0.43)+(0.57)</em>(0.57)=0.51</li>
      <li>Gini for sub-node Class X = (0.56)<em>(0.56)+(0.44)</em>(0.44)=0.51</li>
      <li>Weighted Gini for Split Class = (14/30)<em>0.51+(16/30)</em>0.51 = 0.51</li>
    </ul>
  </li>
</ul>

<h2 id="4-bias-and-variance">4. Bias and Variance</h2>

<p><strong>Bias</strong> error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).</p>

<p><strong>Variance</strong> is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting).</p>

<p><strong>Bias–variance trade-off</strong>  is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set.</p>

<p>Decision trees suffer from bias and variance, having a large bias with simple trees and a large variance with complex trees.</p>

<p><img src="attachment:036e2b46-4638-40f4-8958-91307e72a0a9.png" alt="imagen.png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

      <footer>
        
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
  </body>
</html>
